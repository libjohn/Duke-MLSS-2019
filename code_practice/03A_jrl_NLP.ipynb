{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03A_jrl_NLP.ipynb","version":"0.3.2","provenance":[{"file_id":"https://github.com/jsigman/Duke-MLSS-2019/blob/master/03A_NLP.ipynb","timestamp":1560948562588}],"toc_visible":true},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NmUO1XUSx81u","colab_type":"text"},"source":["# Introduction to Natural Language Processing (NLP) in TensorFlow"]},{"cell_type":"markdown","metadata":{"id":"PHw2JKB_x814","colab_type":"text"},"source":["### Word Embeddings"]},{"cell_type":"markdown","metadata":{"id":"WO3kNG8cx818","colab_type":"text"},"source":["Word embeddings, or word vectors, provide a way of mapping words from a vocabulary into a low-dimensional space, where words with similar meanings are close together. Let's play around with a set of pre-trained word vectors, to get used to their properties. There exist many sets of pretrained word embeddings; here, we use ConceptNet Numberbatch, which provides a relatively small download in an easy-to-work-with format (h5)."]},{"cell_type":"code","metadata":{"id":"FNAGHDFBx81_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"17a8351d-12ed-4469-93b9-8c99096f603f","executionInfo":{"status":"ok","timestamp":1560971816295,"user_tz":240,"elapsed":8106,"user":{"displayName":"John Little","photoUrl":"https://lh3.googleusercontent.com/-NWj4Sz9PIsw/AAAAAAAAAAI/AAAAAAAAATY/fiPj75SQiwU/s64/photo.jpg","userId":"12869586601455934083"}}},"source":["# Download word vectors\n","from urllib.request import urlretrieve\n","import os\n","if not os.path.isfile('mini.h5'):\n","    print(\"Downloading Conceptnet Numberbatch word embeddings...\")\n","    conceptnet_url = 'http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/17.06/mini.h5'\n","    urlretrieve(conceptnet_url, 'mini.h5')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Downloading Conceptnet Numberbatch word embeddings...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HEWCapx4N-yv","colab_type":"code","colab":{}},"source":["#import urllib2\n","#response = urllib2.urlopen('https://raw.githubusercontent.com/jsigman/Duke-MLSS-2019/master/movie-simple.txt')\n","#data = response.read()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mdevcTO9Ox99","colab_type":"code","colab":{}},"source":["#!pip install -q urllib2\n","#!apt-get -qq install -y urllib2\n","#import urllib2\n","###files.download('https://wordpress.org/plugins/about/readme.txt') \n","#response = urllib2.urlopen('https://wordpress.org/plugins/about/readme.txt')\n","#data = response.read()\n","#files.download('https://raw.githubusercontent.com/jsigman/Duke-MLSS-2019/master/movie-simple.txt')\n","#from google.colab import files\n","#files.download('https://github.com/jsigman/Duke-MLSS-2019/blob/master/movie-simple.txt')\n","#uploaded = files.upload()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jdMHFCkzx86r","colab_type":"text"},"source":["To read an `h5` file, we'll need to use the `h5py` package. Below, we use the package to open the `mini.h5` file we just downloaded. We extract from the file a list of utf-8-encoded words, as well as their $300$-dimensional vectors."]},{"cell_type":"code","metadata":{"id":"TmbDOd9vx86t","colab_type":"code","outputId":"d5342318-9de5-4371-d73b-1a9b85654db7","executionInfo":{"status":"ok","timestamp":1560974275934,"user_tz":240,"elapsed":619,"user":{"displayName":"John Little","photoUrl":"https://lh3.googleusercontent.com/-NWj4Sz9PIsw/AAAAAAAAAAI/AAAAAAAAATY/fiPj75SQiwU/s64/photo.jpg","userId":"12869586601455934083"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["# Load the file and pull out words and embeddings\n","import h5py\n","\n","with h5py.File('mini.h5', 'r') as f:\n","    all_words = [word.decode('utf-8') for word in f['mat']['axis1'][:]]\n","    all_embeddings = f['mat']['block0_values'][:]\n","    \n","print(\"all_words dimensions: {0}\".format(len(all_words)))\n","print(\"all_embeddings dimensions: {0}\".format(all_embeddings.shape))\n","\n","print(all_words[10000])"],"execution_count":17,"outputs":[{"output_type":"stream","text":["all_words dimensions: 362891\n","all_embeddings dimensions: (362891, 300)\n","/c/de/lande\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Xa-zQYwpx87B","colab_type":"text"},"source":["Now, `all_words` is a list of $V$ strings (what we call our *vocabulary*), and `all_embeddings` is a $V \\times 300$ matrix. The strings are of the form `/c/language_code/word`â€”for example, `/c/en/cat` and `/c/es/gato`.\n","\n","We are interested only in the English words. We use Python list comprehensions to pull out the indices of the English words, then extract just the English words (stripping the six-character `/c/en/` prefix) and their embeddings."]},{"cell_type":"code","metadata":{"id":"NNm9-wmQx87N","colab_type":"code","outputId":"6cc255cd-8d87-49f0-f505-9f3048c317ce","executionInfo":{"status":"ok","timestamp":1560974764317,"user_tz":240,"elapsed":364,"user":{"displayName":"John Little","photoUrl":"https://lh3.googleusercontent.com/-NWj4Sz9PIsw/AAAAAAAAAAI/AAAAAAAAATY/fiPj75SQiwU/s64/photo.jpg","userId":"12869586601455934083"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["# Restrict our vocabulary to just the English words\n","english_words = [word[6:] for word in all_words if word.startswith('/c/en/')]\n","english_word_indices = [i for i, word in enumerate(all_words) if word.startswith('/c/en/')]\n","english_embeddings = all_embeddings[english_word_indices]\n","\n","print(\"all_words dimensions: {0}\".format(len(english_words)))\n","print(\"all_embeddings dimensions: {0}\".format(english_embeddings.shape))\n","\n","print(english_words[10000])"],"execution_count":19,"outputs":[{"output_type":"stream","text":["all_words dimensions: 150875\n","all_embeddings dimensions: (150875, 300)\n","bajillion\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"imtCvvXix87f","colab_type":"text"},"source":["The magnitude of a word vector is less important than its direction; the magnitude can be thought of as representing frequency of use, independent of the semantics of the word. \n","Here, we will be interested in semantics, so we *normalize* our vectors, dividing each by its length. \n","The result is that all of our word vectors are length 1, and as such, lie on a unit circle. \n","The dot product of two vectors is proportional to the cosine of the angle between them, and provides a measure of similarity (the bigger the cosine, the smaller the angle).\n","\n","<img src=\"https://github.com/jsigman/Duke-MLSS-2019/blob/master/Figures/cosine_similarity.png?raw=1\" alt=\"cosine\" style=\"width: 500px;\"/>\n","<center>Figure adapted from *[Mastering Machine Learning with Spark 2.x](https://www.safaribooksonline.com/library/view/mastering-machine-learning/9781785283451/ba8bef27-953e-42a4-8180-cea152af8118.xhtml)*</center>"]},{"cell_type":"code","metadata":{"id":"_vYU-kdBx87n","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","norms = np.linalg.norm(english_embeddings, axis=1)\n","normalized_embeddings = english_embeddings.astype('float32') / norms.astype('float32').reshape([-1, 1])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vCcD6Z16x87v","colab_type":"text"},"source":["We want to look up words easily, so we create a dictionary that maps us from a word to its index in the word embeddings matrix."]},{"cell_type":"code","metadata":{"id":"UkrtHuGGx87y","colab_type":"code","colab":{}},"source":["index = {word: i for i, word in enumerate(english_words)}\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HzElAJruYCLI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"68335b61-d66d-41b6-c8f5-0c0f61d118c0","executionInfo":{"status":"ok","timestamp":1560975345293,"user_tz":240,"elapsed":281,"user":{"displayName":"John Little","photoUrl":"https://lh3.googleusercontent.com/-NWj4Sz9PIsw/AAAAAAAAAAI/AAAAAAAAATY/fiPj75SQiwU/s64/photo.jpg","userId":"12869586601455934083"}}},"source":["english_words[10000]"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'bajillion'"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"2GPvSRQGYSUl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b2cbbbca-b4ba-4d95-fb15-5e411195fb53","executionInfo":{"status":"ok","timestamp":1560975353476,"user_tz":240,"elapsed":251,"user":{"displayName":"John Little","photoUrl":"https://lh3.googleusercontent.com/-NWj4Sz9PIsw/AAAAAAAAAAI/AAAAAAAAATY/fiPj75SQiwU/s64/photo.jpg","userId":"12869586601455934083"}}},"source":["index['bajillion']"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10000"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"225Yrzm1x877","colab_type":"text"},"source":["Now we are ready to measure the similarity between pairs of words. We use numpy to take dot products."]},{"cell_type":"code","metadata":{"id":"CgP8yQOux87-","colab_type":"code","colab":{}},"source":["def similarity_score(w1, w2):\n","    score = np.dot(normalized_embeddings[index[w1], :], normalized_embeddings[index[w2], :])\n","    return score"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jA4HZ_vKx88H","colab_type":"code","outputId":"06ae8224-3e30-4aa5-e427-b9677b4ca96a","executionInfo":{"status":"ok","timestamp":1560975366898,"user_tz":240,"elapsed":335,"user":{"displayName":"John Little","photoUrl":"https://lh3.googleusercontent.com/-NWj4Sz9PIsw/AAAAAAAAAAI/AAAAAAAAATY/fiPj75SQiwU/s64/photo.jpg","userId":"12869586601455934083"}},"colab":{"base_uri":"https://localhost:8080/","height":140}},"source":["# A word is as similar with itself as possible:\n","print('cat\\tcat\\t', similarity_score('cat', 'cat'))\n","\n","# Closely related words still get high scores:\n","print('cat\\tfeline\\t', similarity_score('cat', 'feline'))\n","print('cat\\tdog\\t', similarity_score('cat', 'dog'))\n","\n","# Unrelated words, not so much\n","print('cat\\tmoo\\t', similarity_score('cat', 'moo'))\n","print('cat\\tfreeze\\t', similarity_score('cat', 'freeze'))\n","\n","# Antonyms are still considered related, sometimes more so than synonyms\n","print('antonyms\\topposites\\t', similarity_score('antonym', 'opposite'))\n","print('antonyms\\tsynonyms\\t', similarity_score('antonym', 'synonym'))"],"execution_count":32,"outputs":[{"output_type":"stream","text":["cat\tcat\t 1.0000001\n","cat\tfeline\t 0.8199548\n","cat\tdog\t 0.590724\n","cat\tmoo\t 0.0039538303\n","cat\tfreeze\t -0.030225191\n","antonyms\topposites\t 0.3941065\n","antonyms\tsynonyms\t 0.46883982\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9XKQti8Ux88W","colab_type":"text"},"source":["We can also find, for instance, the most similar words to a given word."]},{"cell_type":"code","metadata":{"id":"pLmHFlXzx88Z","colab_type":"code","colab":{}},"source":["def closest_to_vector(v, n):\n","    all_scores = np.dot(normalized_embeddings, v)\n","    best_words = map(lambda i: english_words[i], reversed(np.argsort(all_scores)))\n","    return [next(best_words) for _ in range(n)]\n","\n","def most_similar(w, n):\n","    return closest_to_vector(normalized_embeddings[index[w], :], n)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"79XzE2_Kx88i","colab_type":"code","outputId":"7a208122-eed2-4083-f688-8905ad2284dc","executionInfo":{"status":"ok","timestamp":1560975716246,"user_tz":240,"elapsed":265,"user":{"displayName":"John Little","photoUrl":"https://lh3.googleusercontent.com/-NWj4Sz9PIsw/AAAAAAAAAAI/AAAAAAAAATY/fiPj75SQiwU/s64/photo.jpg","userId":"12869586601455934083"}},"colab":{"base_uri":"https://localhost:8080/","height":90}},"source":["print(most_similar('cat', 10))\n","print(most_similar('dog', 10))\n","print(most_similar('duke', 10))"],"execution_count":35,"outputs":[{"output_type":"stream","text":["['cat', 'humane_society', 'kitten', 'feline', 'colocolo', 'cats', 'kitty', 'maine_coon', 'housecat', 'sharp_teeth']\n","['dog', 'dogs', 'wire_haired_dachshund', 'doggy_paddle', 'lhasa_apso', 'good_friend', 'puppy_dog', 'bichon_frise', 'woof_woof', 'golden_retrievers']\n","['duke', 'dukes', 'duchess', 'duchesses', 'ducal', 'dukedom', 'duchy', 'voivode', 'princes', 'prince']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OGYc5-fCx88q","colab_type":"text"},"source":["**Can you find a polysemous word â€” a word with multiple meanings â€” so that the list of the top 10 most related words contains words that aren't themselves related to one another?**"]},{"cell_type":"code","metadata":{"id":"D9rfuBwPx88t","colab_type":"code","outputId":"a35d95ec-5889-4862-b4a4-9a59ec138de6","executionInfo":{"status":"ok","timestamp":1560975815587,"user_tz":240,"elapsed":359,"user":{"displayName":"John Little","photoUrl":"https://lh3.googleusercontent.com/-NWj4Sz9PIsw/AAAAAAAAAAI/AAAAAAAAATY/fiPj75SQiwU/s64/photo.jpg","userId":"12869586601455934083"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["print(most_similar('gusto', 10))\n","print(most_similar('right', 10))\n","print(most_similar('cleave', 10))\n","print(most_similar('ass', 10))\n","print(most_similar('bad', 10))\n","print(most_similar('break', 10))"],"execution_count":36,"outputs":[{"output_type":"stream","text":["['gusto', 'enthusiasm', 'zest', 'zeal', 'delight', 'fervor', 'relish', 'vigor', 'enthusiasms', 'ardor']\n","['right', 'righted', 'religious_right', 'rightmost', 'rightward', 'rightwards', 'righting', 'dextral', 'righ', 'rightie']\n","['cleave', 'cleaved', 'cleaves', 'cleaving', 'scission', 'rend', 'split', 'cleaver', 'divide', 'sever']\n","['ass', 'asses', 'arse', 'butt', 'butts', 'arses', 'assed', 'azz', 'bunda', 'buttocks']\n","['bad', 'woe_betide', 'badder', 'baddest', 'from_hell', 'lousy', 'worst', 'riaa', 'unpleasant', 'worse']\n","['break', 'breaks', 'breaked', 'breaking', 'broke', 'shatter', 'broken', 'breakage', 'shatters', 'smash']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fEmDU6ypx88z","colab_type":"text"},"source":["We can also use `closest_to_vector` to find words \"nearby\" vectors that we create ourselves. This allows us to solve analogies. For example, in order to solve the analogy \"man : brother :: woman : ?\", we can compute a new vector `brother - man + woman`: the meaning of brother, minus the meaning of man, plus the meaning of woman. We can then ask which words are closest, in the embedding space, to that new vector."]},{"cell_type":"code","metadata":{"id":"MjmUB9kFx884","colab_type":"code","outputId":"71b716c2-cb45-48ab-b190-eacd3847142b","executionInfo":{"status":"ok","timestamp":1560976612549,"user_tz":240,"elapsed":303,"user":{"displayName":"John Little","photoUrl":"https://lh3.googleusercontent.com/-NWj4Sz9PIsw/AAAAAAAAAAI/AAAAAAAAATY/fiPj75SQiwU/s64/photo.jpg","userId":"12869586601455934083"}},"colab":{"base_uri":"https://localhost:8080/","height":90}},"source":["def solve_analogy(a1, b1, a2):\n","    b2 = normalized_embeddings[index[b1], :] - normalized_embeddings[index[a1], :] + normalized_embeddings[index[a2], :]\n","    return closest_to_vector(b2, 10)\n","\n","print(solve_analogy(\"man\", \"brother\", \"woman\"))\n","print(solve_analogy(\"man\", \"husband\", \"woman\"))\n","print(solve_analogy(\"spain\", \"madrid\", \"france\"))"],"execution_count":38,"outputs":[{"output_type":"stream","text":["['sister', 'brother', 'sisters', 'kid_sister', 'younger_brother', 'niece', 'nieces', 'sistren', 'stepsister', 'daughter']\n","['wife', 'husband', 'husbands', 'spouse', 'wifes', 'wifey', 'et_ux', 'hubby', 'hotwife', 'wives']\n","['paris', 'france', 'le_havre', 'in_france', 'montmartre', 'marseille', 'loire_valley', 'saone', 'lyonnais', 'jacques_chirac']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Vnz9oBCJx89A","colab_type":"text"},"source":["**Note that the new vector, `b2`, is not normalized. Does this matter?  Why or why not?**"]},{"cell_type":"markdown","metadata":{"id":"fFKiOSk-x89D","colab_type":"text"},"source":["These three results are quite good, but in general, the results of these analogies can be disappointing. Try experimenting with other analogies, and see if you can think of ways to get around the problems you notice (i.e., modifications to the solve_analogy algorithm)."]},{"cell_type":"markdown","metadata":{"id":"WHpKOycrx89H","colab_type":"text"},"source":["### Using word embeddings in deep models\n","Word embeddings are fun to play around with, but their primary use is that they allow us to think of words as existing in a continuous, Euclidean space; we can then use an existing arsenal of techniques for machine learning with continuous numerical data (like logistic regression or neural networks) to process text.\n","\n","Let's take a look at an especially simple version of this. We'll perform *sentiment analysis* on a set of movie reviews: in particular, we will attempt to classify a movie review as positive or negative based on its text.\n","\n","We will use a [Simple Word Embedding Model](http://people.ee.duke.edu/~lcarin/acl2018_swem.pdf) (SWEM, Shen et al. 2018) to do so. We will represent a review as the *mean* of the embeddings of the words in the review. Then we'll train a three-layer MLP (a neural network) to classify the review as positive or negative.\n","\n","Download the `movie-simple.txt` file from Google Classroom into this directory. Each line of that file contains \n","\n","1. the numeral 0 (for negative) or the numeral 1 (for positive), followed by\n","2. a tab (the whitespace character), and then\n","3. the review itself."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"NRMjcF_yx89L","colab_type":"code","colab":{}},"source":["import string\n","remove_punct=str.maketrans('','',string.punctuation)\n","\n","# This function converts a line of our data file into\n","# a tuple (x, y), where x is 300-dimensional representation\n","# of the words in a review, and y is its label.\n","def convert_line_to_example(line):\n","    # Pull out the first character: that's our label (0 or 1)\n","    y = int(line[0])\n","    \n","    # Split the line into words using Python's split() function\n","    words = line[2:].translate(remove_punct).lower().split()\n","    \n","    # Look up the embeddings of each word, ignoring words not\n","    # in our pretrained vocabulary.\n","    embeddings = [normalized_embeddings[index[w]] for w in words\n","                  if w in index]\n","    \n","    # Take the mean of the embeddings\n","    x = np.mean(np.vstack(embeddings), axis=0)\n","    return {'x': x, 'y': y}\n","\n","# Apply the function to each line in the file.\n","# manually uploaded 'movie-simple.txt' to the File-space (from Table of Contents) from Github\n","# https://raw.githubusercontent.com/jsigman/Duke-MLSS-2019/master/movie-simple.txt\n","# sometime soon I'll figure out how to get Python to do this for me.\n","with open(\"movie-simple.txt\", \"r\", encoding='utf-8', errors='ignore') as f:\n","    dataset = [convert_line_to_example(l) for l in f.readlines()]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LxjT0oTvx89S","colab_type":"code","outputId":"0fa585db-85ed-447d-e212-8ae438848ba5","executionInfo":{"status":"ok","timestamp":1560976622923,"user_tz":240,"elapsed":283,"user":{"displayName":"John Little","photoUrl":"https://lh3.googleusercontent.com/-NWj4Sz9PIsw/AAAAAAAAAAI/AAAAAAAAATY/fiPj75SQiwU/s64/photo.jpg","userId":"12869586601455934083"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(dataset)"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1411"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"markdown","metadata":{"id":"FZLT2X0Cx89b","colab_type":"text"},"source":["Now that we have a dataset, let's shuffle it and do a train/test split. We use a quarter of the dataset for testing, 3/4 for training (but also ensure that we have a whole number of batches in our training set, to make the code nicer later)."]},{"cell_type":"code","metadata":{"id":"8tL_mO51x89g","colab_type":"code","colab":{}},"source":["import random\n","random.shuffle(dataset)\n","\n","batch_size = 100\n","total_batches = len(dataset) // batch_size\n","train_batches = 3*total_batches // 4 \n","train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ypxTn72ax89p","colab_type":"text"},"source":["Time to build our MLP in Tensorflow. We'll use placeholders for `X` and `y` as usual."]},{"cell_type":"code","metadata":{"id":"mhJEqfBRx89r","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":213},"outputId":"f08d2d1d-72e1-408f-e77c-97c03cc789f6","executionInfo":{"status":"ok","timestamp":1560976632609,"user_tz":240,"elapsed":1862,"user":{"displayName":"John Little","photoUrl":"https://lh3.googleusercontent.com/-NWj4Sz9PIsw/AAAAAAAAAAI/AAAAAAAAATY/fiPj75SQiwU/s64/photo.jpg","userId":"12869586601455934083"}}},"source":["import tensorflow as tf\n","\n","# Placeholders for input\n","X = tf.placeholder(tf.float32, [None, 300])\n","y = tf.placeholder(tf.float32, [None, 1])\n","\n","# Three-layer MLP\n","h1 = tf.layers.dense(X, 100, tf.nn.relu)\n","h2 = tf.layers.dense(h1, 20, tf.nn.relu)\n","logits = tf.layers.dense(h2, 1)\n","probabilities = tf.sigmoid(logits)\n","\n","# Loss and metrics\n","loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n","accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(probabilities), y), tf.float32))\n","\n","# Training\n","train_step = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n","\n","# Initialization of variables\n","init_op = tf.global_variables_initializer()"],"execution_count":42,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0619 20:37:12.442302 139744459282304 deprecation.py:323] From <ipython-input-42-bd1124f959fe>:8: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dense instead.\n","W0619 20:37:12.448512 139744459282304 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","W0619 20:37:12.767754 139744459282304 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"xdSgv376x892","colab_type":"text"},"source":["We can now begin a session and train our model. We'll train for 250 epochs. When we're finished, we'll evaluate our accuracy on all the test data."]},{"cell_type":"code","metadata":{"id":"0Bm3J3Cjx897","colab_type":"code","outputId":"f7c41e0a-31ce-4c57-f53b-8ab7649a3818","executionInfo":{"status":"ok","timestamp":1560977047613,"user_tz":240,"elapsed":8286,"user":{"displayName":"John Little","photoUrl":"https://lh3.googleusercontent.com/-NWj4Sz9PIsw/AAAAAAAAAAI/AAAAAAAAATY/fiPj75SQiwU/s64/photo.jpg","userId":"12869586601455934083"}},"colab":{"base_uri":"https://localhost:8080/","height":474}},"source":["# Train\n","sess = tf.Session()\n","sess.run(init_op)\n","\n","for epoch in range(250):\n","    for batch in range(train_batches):\n","        data = train[batch*batch_size:(batch+1)*batch_size]\n","        reviews = [sample['x'] for sample in data]\n","        labels  = [sample['y'] for sample in data]\n","        labels = np.array(labels).reshape([-1,1])\n","        \n","        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n","        \n","    if epoch % 10 == 0:\n","        print(\"Epoch: {0} \\t Loss: {1} \\t Acc: {2}\".format(epoch, l, acc))\n","    \n","    random.shuffle(train)\n","        \n","# Evaluate on test set\n","test_reviews = [sample['x'] for sample in test]\n","test_labels  = [sample['y'] for sample in test]\n","test_labels  = np.array(test_labels).reshape([-1, 1])\n","\n","acc = sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n","print(\"Final accuracy: {0}\".format(acc))"],"execution_count":43,"outputs":[{"output_type":"stream","text":["Epoch: 0 \t Loss: 0.6946114301681519 \t Acc: 0.46000000834465027\n","Epoch: 10 \t Loss: 0.6793479323387146 \t Acc: 0.5600000023841858\n","Epoch: 20 \t Loss: 0.6740855574607849 \t Acc: 0.5799999833106995\n","Epoch: 30 \t Loss: 0.6611305475234985 \t Acc: 0.6299999952316284\n","Epoch: 40 \t Loss: 0.6463670134544373 \t Acc: 0.6299999952316284\n","Epoch: 50 \t Loss: 0.6332020163536072 \t Acc: 0.6700000166893005\n","Epoch: 60 \t Loss: 0.600393533706665 \t Acc: 0.7400000095367432\n","Epoch: 70 \t Loss: 0.5740084648132324 \t Acc: 0.8100000023841858\n","Epoch: 80 \t Loss: 0.5156574845314026 \t Acc: 0.8299999833106995\n","Epoch: 90 \t Loss: 0.44840899109840393 \t Acc: 0.8899999856948853\n","Epoch: 100 \t Loss: 0.3735537827014923 \t Acc: 0.8799999952316284\n","Epoch: 110 \t Loss: 0.3248090445995331 \t Acc: 0.9100000262260437\n","Epoch: 120 \t Loss: 0.2852449119091034 \t Acc: 0.9300000071525574\n","Epoch: 130 \t Loss: 0.20646663010120392 \t Acc: 0.949999988079071\n","Epoch: 140 \t Loss: 0.20869185030460358 \t Acc: 0.9399999976158142\n","Epoch: 150 \t Loss: 0.19813886284828186 \t Acc: 0.9100000262260437\n","Epoch: 160 \t Loss: 0.1502813845872879 \t Acc: 0.949999988079071\n","Epoch: 170 \t Loss: 0.15743929147720337 \t Acc: 0.9599999785423279\n","Epoch: 180 \t Loss: 0.20562610030174255 \t Acc: 0.9100000262260437\n","Epoch: 190 \t Loss: 0.11936906725168228 \t Acc: 0.9599999785423279\n","Epoch: 200 \t Loss: 0.1861209124326706 \t Acc: 0.8799999952316284\n","Epoch: 210 \t Loss: 0.12735866010189056 \t Acc: 0.949999988079071\n","Epoch: 220 \t Loss: 0.09430323541164398 \t Acc: 0.9700000286102295\n","Epoch: 230 \t Loss: 0.10060037672519684 \t Acc: 0.9800000190734863\n","Epoch: 240 \t Loss: 0.07825278490781784 \t Acc: 0.9800000190734863\n","Final accuracy: 0.9367396831512451\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IjLEUsIfx8-L","colab_type":"text"},"source":["We can now examine what our model has learned, seeing how it responds to word vectors for different words:"]},{"cell_type":"code","metadata":{"id":"NJMNLcSRx8-N","colab_type":"code","outputId":"08c5397c-3b9c-4726-d5ed-c568ba58b1e0","executionInfo":{"status":"ok","timestamp":1560977086290,"user_tz":240,"elapsed":343,"user":{"displayName":"John Little","photoUrl":"https://lh3.googleusercontent.com/-NWj4Sz9PIsw/AAAAAAAAAAI/AAAAAAAAATY/fiPj75SQiwU/s64/photo.jpg","userId":"12869586601455934083"}},"colab":{"base_uri":"https://localhost:8080/","height":140}},"source":["# Check some words\n","words_to_test = [\"exciting\", \"hated\", \"boring\", \"loved\", \"extremely\", \"rather\", \"quite\"]\n","\n","for word in words_to_test:\n","    print(word, sess.run(probabilities, feed_dict={X: normalized_embeddings[index[word]].reshape(1, 300)}))"],"execution_count":44,"outputs":[{"output_type":"stream","text":["exciting [[0.9999918]]\n","hated [[9.049819e-07]]\n","boring [[9.248749e-06]]\n","loved [[1.]]\n","extremely [[0.504851]]\n","rather [[0.11691659]]\n","quite [[0.9281063]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"y4N8mp2zx8-U","colab_type":"text"},"source":["Try some words of your own!"]},{"cell_type":"code","metadata":{"id":"YXW2UmQrx8-X","colab_type":"code","colab":{}},"source":["sess.close()\n","tf.reset_default_graph()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xXLxXfXax8-h","colab_type":"text"},"source":["This model works great for such a simple dataset, but does a little less well on something more complex. `movie-pang02.txt`, for instance, has 2000 longer, more complex movie reviews. It's in the same format as our simple dataset. On those longer reviews, this model achieves only 60-80% accuracy. (Increasing the number of epochs to, say, 1000, does help.)"]}]}